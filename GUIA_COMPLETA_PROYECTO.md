# GuÃ­a Completa del Proyecto ETL - E-Commerce Olist

## ğŸ“‹ Ãndice

1. [DescripciÃ³n General del Proyecto](#descripciÃ³n-general-del-proyecto)
2. [Estructura del Proyecto](#estructura-del-proyecto)
3. [TecnologÃ­as Utilizadas](#tecnologÃ­as-utilizadas)
4. [Fases del Pipeline ETL](#fases-del-pipeline-etl)
5. [ConfiguraciÃ³n e InstalaciÃ³n](#configuraciÃ³n-e-instalaciÃ³n)
6. [ExplicaciÃ³n Detallada de Cada Componente](#explicaciÃ³n-detallada-de-cada-componente)
7. [OrquestaciÃ³n con Apache Airflow](#orquestaciÃ³n-con-apache-airflow)
8. [Pruebas y ValidaciÃ³n](#pruebas-y-validaciÃ³n)
9. [Visualizaciones](#visualizaciones)
10. [EjecuciÃ³n Paso a Paso](#ejecuciÃ³n-paso-a-paso)

---

## ğŸ“Š DescripciÃ³n General del Proyecto

Este proyecto implementa un **pipeline de datos ETL (Extract, Transform, Load)** para analizar datos de comercio electrÃ³nico de la empresa brasileÃ±a **Olist**. El objetivo es ayudar a una de las plataformas de e-commerce mÃ¡s grandes de LatinoamÃ©rica a analizar su rendimiento en mÃ©tricas clave durante los aÃ±os 2016-2018.

### ğŸ¯ Objetivos del Negocio

- **AnÃ¡lisis de Ingresos**: Entender cuÃ¡ntos ingresos se generaron por aÃ±o, categorÃ­as mÃ¡s y menos populares, ingresos por estado
- **AnÃ¡lisis de Entregas**: Evaluar quÃ© tan bien la empresa estÃ¡ entregando productos a tiempo, tiempos de entrega segÃºn el mes, diferencias entre fecha estimada y real de entrega

### ğŸ“ˆ MÃ©tricas Principales

- Ingresos por aÃ±o, mes y estado
- Top 10 de categorÃ­as con mayor/menor ingresos
- Diferencias en fechas de entrega
- RelaciÃ³n entre peso y costo de envÃ­o
- AnÃ¡lisis de pedidos vs dÃ­as festivos

---

## ğŸ—‚ï¸ Estructura del Proyecto

```
assignmentProyect01/
â”œâ”€â”€ ğŸ“„ README.md                    # DocumentaciÃ³n principal
â”œâ”€â”€ ğŸ“„ ASSIGNMENT.md                # Instrucciones del proyecto
â”œâ”€â”€ ğŸ“„ requirements.txt             # Dependencias Python
â”œâ”€â”€ ğŸ“„ ETLProject.ipynb             # Notebook principal para anÃ¡lisis
â”œâ”€â”€ ğŸ“„ olist.db                     # Base de datos SQLite generada
â”œâ”€â”€
â”œâ”€â”€ ğŸ“ dataset/                     # Datos fuente (CSV)
â”‚   â”œâ”€â”€ olist_customers_dataset.csv
â”‚   â”œâ”€â”€ olist_orders_dataset.csv
â”‚   â”œâ”€â”€ olist_order_items_dataset.csv
â”‚   â”œâ”€â”€ olist_order_payments_dataset.csv
â”‚   â”œâ”€â”€ olist_order_reviews_dataset.csv
â”‚   â”œâ”€â”€ olist_products_dataset.csv
â”‚   â”œâ”€â”€ olist_sellers_dataset.csv
â”‚   â”œâ”€â”€ olist_geolocation_dataset.csv
â”‚   â””â”€â”€ product_category_name_translation.csv
â”œâ”€â”€
â”œâ”€â”€ ğŸ“ src/                         # CÃ³digo fuente principal
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                   # Configuraciones del proyecto
â”‚   â”œâ”€â”€ extract.py                  # MÃ³dulo de extracciÃ³n de datos
â”‚   â”œâ”€â”€ load.py                     # MÃ³dulo de carga a base de datos
â”‚   â”œâ”€â”€ transform.py                # MÃ³dulo de transformaciÃ³n y consultas
â”‚   â””â”€â”€ plots.py                    # MÃ³dulo de visualizaciones
â”œâ”€â”€
â”œâ”€â”€ ğŸ“ queries/                     # Consultas SQL
â”‚   â”œâ”€â”€ delivery_date_difference.sql
â”‚   â”œâ”€â”€ global_ammount_order_status.sql
â”‚   â”œâ”€â”€ revenue_by_month_year.sql
â”‚   â”œâ”€â”€ revenue_per_state.sql
â”‚   â”œâ”€â”€ top_10_least_revenue_categories.sql
â”‚   â”œâ”€â”€ top_10_revenue_categories.sql
â”‚   â””â”€â”€ real_vs_estimated_delivered_time.sql
â”œâ”€â”€
â”œâ”€â”€ ğŸ“ tests/                       # Pruebas unitarias
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â”œâ”€â”€ test_transform.py
â”‚   â””â”€â”€ query_results/              # Resultados esperados
â”œâ”€â”€
â”œâ”€â”€ ğŸ“ airflow_home/                # ConfiguraciÃ³n de Airflow
â”‚   â”œâ”€â”€ airflow.cfg
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ elt_olist_dag.py        # DAG de Airflow
â”œâ”€â”€
â”œâ”€â”€ ğŸ“ images/                      # ImÃ¡genes y grÃ¡ficos
â”œâ”€â”€ ğŸ“ exports/                     # Resultados exportados (CSV)
â””â”€â”€ ğŸ“ myenvname/                   # Entorno virtual Python
```

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

| TecnologÃ­a             | PropÃ³sito                          | VersiÃ³n |
| ---------------------- | ---------------------------------- | ------- |
| **Python**             | Lenguaje principal de programaciÃ³n | 3.8+    |
| **Pandas**             | ManipulaciÃ³n y anÃ¡lisis de datos   | Ãšltima  |
| **SQLite**             | Base de datos para Data Warehouse  | -       |
| **SQLAlchemy**         | ORM y conexiÃ³n a base de datos     | 2.x     |
| **Requests**           | Consumo de APIs REST               | Ãšltima  |
| **Matplotlib/Seaborn** | VisualizaciÃ³n de datos             | Ãšltima  |
| **Jupyter**            | Notebooks interactivos             | Ãšltima  |
| **Apache Airflow**     | OrquestaciÃ³n de pipelines          | 2.9.1   |
| **Pytest**             | Framework de pruebas               | Ãšltima  |
| **Black**              | Formateo automÃ¡tico de cÃ³digo      | Ãšltima  |

---

## ğŸ”„ Fases del Pipeline ETL

### 1. **EXTRACT (ExtracciÃ³n)**

- **Fuente 1**: Archivos CSV del dataset de Olist (datos locales)
- **Fuente 2**: API pÃºblica de dÃ­as festivos de Brasil (https://date.nager.at)
- **Resultado**: DataFrames en memoria con datos limpios

### 2. **LOAD (Carga)**

- **Destino**: Base de datos SQLite (`olist.db`)
- **Proceso**: Carga de todos los DataFrames como tablas
- **Estrategia**: Reemplazar tablas existentes (`if_exists='replace'`)

### 3. **TRANSFORM (TransformaciÃ³n)**

- **Herramienta**: Consultas SQL personalizadas
- **Proceso**: AnÃ¡lisis y agregaciÃ³n de datos
- **Resultado**: MÃ©tricas de negocio calculadas

---

## âš™ï¸ ConfiguraciÃ³n e InstalaciÃ³n

### Paso 1: Crear Entorno Virtual

```bash
# Crear entorno virtual
python -m venv myenvname

# Activar entorno (macOS/Linux)
source myenvname/bin/activate

# Activar entorno (Windows)
myenvname\Scripts\activate
```

### Paso 2: Instalar Dependencias

```bash
pip install -r requirements.txt
```

### Paso 3: Verificar InstalaciÃ³n

```bash
# Ejecutar pruebas
pytest tests/

# Formatear cÃ³digo
black --line-length=88 .
```

---

## ğŸ” ExplicaciÃ³n Detallada de Cada Componente

### ğŸ“ `src/config.py`

Archivo de configuraciÃ³n central que define:

- Rutas de archivos y directorios
- URLs de APIs
- Mapeo de archivos CSV a nombres de tablas
- Configuraciones de base de datos

```python
# Ejemplos de configuraciÃ³n
DATASET_ROOT_PATH = "dataset"
SQLITE_BD_ABSOLUTE_PATH = "olist.db"
PUBLIC_HOLIDAYS_URL = "https://date.nager.at/api/v3/publicholidays"
CSV_TABLE_MAPPING = {
    "olist_customers_dataset.csv": "olist_customers",
    "olist_orders_dataset.csv": "olist_orders",
    # ... mÃ¡s mapeos
}
```

### ğŸ“ `src/extract.py`

**Funciones principales:**

#### `get_public_holidays(public_holidays_url, year)`

- **PropÃ³sito**: Obtiene dÃ­as festivos de Brasil desde API externa
- **Entrada**: URL de API y aÃ±o especÃ­fico
- **Proceso**:
  1. Hace peticiÃ³n HTTP a la API
  2. Convierte respuesta JSON a DataFrame
  3. Elimina columnas innecesarias (`types`, `counties`)
  4. Convierte columna `date` a formato datetime
- **Salida**: DataFrame con dÃ­as festivos

#### `extract(csv_folder, csv_table_mapping, public_holidays_url)`

- **PropÃ³sito**: FunciÃ³n principal de extracciÃ³n
- **Proceso**:
  1. Lee todos los archivos CSV usando pandas
  2. Crea diccionario de DataFrames
  3. Obtiene dÃ­as festivos para 2017
  4. Combina todos los datos
- **Salida**: Diccionario con todos los DataFrames listos para carga

### ğŸ“ `src/load.py`

**FunciÃ³n principal:**

#### `load(data_frames, database)`

- **PropÃ³sito**: Carga DataFrames en base de datos SQLite
- **CaracterÃ­sticas**:
  - Compatible con SQLAlchemy Engine y conexiones DBAPI
  - Usa `raw_connection()` para evitar problemas con SQLAlchemy 2.x
  - Reemplaza tablas existentes (`if_exists='replace'`)
  - Hace commit despuÃ©s de cada tabla
- **GestiÃ³n de errores**: Manejo robusto de conexiones y commits

### ğŸ“ `src/transform.py`

**Componentes principales:**

#### `QueryEnum`

EnumeraciÃ³n que define todas las consultas disponibles:

- `DELIVERY_DATE_DIFFERECE`
- `GLOBAL_AMMOUNT_ORDER_STATUS`
- `REVENUE_BY_MONTH_YEAR`
- `REVENUE_PER_STATE`
- `TOP_10_LEAST_REVENUE_CATEGORIES`
- `TOP_10_REVENUE_CATEGORIES`
- `REAL_VS_ESTIMATED_DELIVERED_TIME`

#### Funciones de consulta

Cada consulta tiene su funciÃ³n dedicada que:

1. Lee el archivo SQL correspondiente
2. Ejecuta la consulta usando `pandas.read_sql`
3. Retorna un `QueryResult` con nombre de consulta y resultado

### ğŸ“ `queries/` - Consultas SQL

#### `revenue_by_month_year.sql`

**PropÃ³sito**: Calcula ingresos mensuales por aÃ±o (2016-2018)
**Estructura**:

- `month_no`: NÃºmero de mes (01-12)
- `month`: Nombre corto del mes (Jan, Feb, etc.)
- `Year2016`, `Year2017`, `Year2018`: Ingresos por aÃ±o

**LÃ³gica SQL**:

```sql
-- Combina Ã³rdenes con pagos
-- Agrupa por mes y aÃ±o
-- Usa PIVOT para mostrar aÃ±os como columnas
-- Rellena con 0.00 cuando no hay datos
```

#### `top_10_revenue_categories.sql`

**PropÃ³sito**: Top 10 categorÃ­as con mayores ingresos
**Incluye**: TraducciÃ³n de nombres de categorÃ­as al inglÃ©s

#### `delivery_date_difference.sql`

**PropÃ³sito**: Analiza diferencias entre fechas estimadas y reales de entrega
**MÃ©tricas**: DÃ­as de diferencia, porcentajes de entregas a tiempo

### ğŸ“ `src/plots.py`

**Funciones de visualizaciÃ³n:**

- `plot_revenue_by_month_year()`: GrÃ¡fico de lÃ­neas de ingresos mensuales
- `plot_top_10_revenue_categories()`: GrÃ¡fico de barras de categorÃ­as top
- `plot_delivery_date_difference()`: Histograma de diferencias de entrega
- `plot_freight_value_weight_relationship()`: Scatter plot peso vs costo envÃ­o
- Y mÃ¡s visualizaciones especializadas...

---

## ğŸŒŠ OrquestaciÃ³n con Apache Airflow

### ğŸ“ `airflow_home/dags/elt_olist_dag.py`

#### Estructura del DAG

```python
DAG: elt_olist_pipeline
â”œâ”€â”€ Task 1: extract_task
â”œâ”€â”€ Task 2: load_task
â”œâ”€â”€ Task 3: transform_task
â””â”€â”€ Task 4: summary_task
```

#### Flujo de Tareas

**1. extract_task**

- Ejecuta funciÃ³n de extracciÃ³n
- Cachea resultados en `.airflow_cache/` como archivos CSV
- Evita pasar DataFrames pesados por XCom

**2. load_task**

- Lee archivos CSV cacheados
- Carga datos en SQLite (`olist.db`)
- Depende de: `extract_task`

**3. transform_task**

- Ejecuta todas las consultas SQL
- Exporta resultados a `exports/*.csv`
- Depende de: `load_task`

**4. summary_task**

- Muestra mÃ©tricas de resumen
- Cuenta registros por tabla
- Depende de: `extract_task`

#### Ventajas del DiseÃ±o

- **Cache inteligente**: Evita re-extracciones innecesarias
- **ReutilizaciÃ³n de cÃ³digo**: Usa mÃ³dulos existentes de `src/`
- **ParÃ¡metros configurables**: `holidays_year` personalizable
- **GestiÃ³n de errores**: Robust error handling en cada tarea

### ConfiguraciÃ³n de Airflow

#### InstalaciÃ³n (Entorno Separado)

```bash
# Crear entorno especÃ­fico para Airflow
python -m venv airflow_venv
source airflow_venv/bin/activate

# Instalar Airflow con constraints
pip install "apache-airflow==2.9.1" --constraint https://raw.githubusercontent.com/apache/airflow/constraints-2.9.1/constraints-3.12.txt

# Inicializar metadatos
airflow db init

# Crear usuario administrador
airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin
```

#### EjecuciÃ³n

```bash
# Terminal 1: Servidor web
airflow webserver -p 8080

# Terminal 2: Scheduler
airflow scheduler

# Acceder a: http://localhost:8080
```

#### EjecuciÃ³n Manual

```bash
# Ejecutar DAG completo
airflow dags trigger elt_olist_pipeline

# Con parÃ¡metros personalizados
airflow dags trigger elt_olist_pipeline --conf '{"holidays_year": 2018}'
```

---

## ğŸ§ª Pruebas y ValidaciÃ³n

### Estructura de Pruebas

```
tests/
â”œâ”€â”€ test_extract.py          # Pruebas de extracciÃ³n
â”œâ”€â”€ test_transform.py        # Pruebas de transformaciÃ³n
â””â”€â”€ query_results/           # Resultados esperados
    â”œâ”€â”€ delivery_date_difference.json
    â”œâ”€â”€ revenue_by_month_year.json
    â””â”€â”€ ...mÃ¡s archivos de referencia
```

### Tipos de Pruebas

#### Pruebas de ExtracciÃ³n (`test_extract.py`)

- Valida conexiÃ³n a API de dÃ­as festivos
- Verifica formato correcto de DataFrames
- Comprueba limpieza y transformaciÃ³n de datos
- Testa manejo de errores de red

#### Pruebas de TransformaciÃ³n (`test_transform.py`)

- Ejecuta cada consulta SQL
- Compara resultados con archivos de referencia JSON
- Valida estructura de datos de salida
- Verifica cÃ¡lculos de mÃ©tricas de negocio

### EjecuciÃ³n de Pruebas

```bash
# Ejecutar todas las pruebas
pytest tests/

# Pruebas especÃ­ficas
pytest tests/test_extract.py
pytest tests/test_transform.py

# Con verbosidad
pytest tests/ -v

# Con reporte de cobertura
pytest tests/ --cov=src/
```

---

## ğŸ“Š Visualizaciones

### Notebook Principal: `ETLProject.ipynb`

#### Estructura del Notebook

1. **ConfiguraciÃ³n inicial**: Imports y setup de base de datos
2. **Fase de ExtracciÃ³n**: EjecuciÃ³n de funciones de extracciÃ³n
3. **Fase de Carga**: Carga de datos en SQLite
4. **Fase de TransformaciÃ³n**: EjecuciÃ³n de consultas SQL
5. **Visualizaciones**: GrÃ¡ficos interactivos con matplotlib/seaborn

#### Tipos de GrÃ¡ficos Generados

**1. GrÃ¡ficos de Ingresos**

- Ingresos mensuales por aÃ±o (lÃ­neas temporales)
- Ingresos por estado (mapas de calor)
- Top 10 categorÃ­as mÃ¡s/menos rentables (barras)

**2. AnÃ¡lisis de Entregas**

- DistribuciÃ³n de diferencias en fechas de entrega
- Tiempo real vs estimado de entrega
- RelaciÃ³n peso-costo de envÃ­o

**3. AnÃ¡lisis Temporal**

- Pedidos por dÃ­a con marcadores de dÃ­as festivos
- Tendencias estacionales

### GeneraciÃ³n de ImÃ¡genes

Las visualizaciones se guardan automÃ¡ticamente en:

```
images/
â”œâ”€â”€ revenue_by_month_year.png
â”œâ”€â”€ top_10_revenue_categories.png
â”œâ”€â”€ delivery_date_difference.png
â”œâ”€â”€ freight_value_weight_relationship.png
â””â”€â”€ orders_per_day_and_holidays.png
```

---

## ğŸš€ EjecuciÃ³n Paso a Paso

### MÃ©todo 1: EjecuciÃ³n Manual (Recomendado para Desarrollo)

#### Paso 1: PreparaciÃ³n del Entorno

```bash
# Navegar al directorio del proyecto
cd /Users/santiagomanchola/Desktop/Universidad/2025B/Inteligencia\ de\ negocios/assignmentProyect01

# Activar entorno virtual
source myenvname/bin/activate

# Verificar instalaciÃ³n
python --version
pip list
```

#### Paso 2: EjecuciÃ³n del Pipeline

```bash
# OpciÃ³n A: Ejecutar notebook completo
jupyter notebook ETLProject.ipynb

# OpciÃ³n B: Ejecutar mÃ³dulos individuales
python -c "
from src.extract import extract
from src.load import load
from src.transform import run_queries
from src.config import *
import pandas as pd
from sqlalchemy import create_engine

# Extract
data = extract(DATASET_ROOT_PATH, CSV_TABLE_MAPPING, PUBLIC_HOLIDAYS_URL)
print(f'Extracted {len(data)} tables')

# Load
engine = create_engine(f'sqlite:///{SQLITE_BD_ABSOLUTE_PATH}')
load(data, engine)
print('Data loaded to SQLite')

# Transform
results = run_queries(engine)
print(f'Executed {len(results)} queries')
"
```

#### Paso 3: ValidaciÃ³n

```bash
# Ejecutar pruebas
pytest tests/ -v

# Verificar archivos generados
ls -la olist.db
ls -la exports/
ls -la images/
```

### MÃ©todo 2: EjecuciÃ³n con Airflow (Recomendado para ProducciÃ³n)

#### Paso 1: Configurar Airflow

```bash
# Activar entorno de Airflow
source airflow_venv/bin/activate

# Verificar configuraciÃ³n
airflow config list
airflow dags list
```

#### Paso 2: Ejecutar DAG

```bash
# Ejecutar desde lÃ­nea de comandos
airflow dags trigger elt_olist_pipeline

# O desde interfaz web
# 1. Abrir http://localhost:8080
# 2. Buscar DAG "elt_olist_pipeline"
# 3. Activar toggle
# 4. Click en "Trigger DAG"
```

#### Paso 3: Monitoreo

```bash
# Ver estado de ejecuciÃ³n
airflow dags state elt_olist_pipeline $(date +%Y-%m-%d)

# Ver logs de tareas
airflow tasks log elt_olist_pipeline extract_task $(date +%Y-%m-%d)
airflow tasks log elt_olist_pipeline load_task $(date +%Y-%m-%d)
airflow tasks log elt_olist_pipeline transform_task $(date +%Y-%m-%d)
```

### MÃ©todo 3: EjecuciÃ³n de Componentes Individuales

#### Solo ExtracciÃ³n

```bash
pytest tests/test_extract.py -v
```

#### Solo TransformaciÃ³n

```bash
pytest tests/test_transform.py -v
```

#### Consulta EspecÃ­fica

```bash
python -c "
from src.transform import query_revenue_by_month_year
from sqlalchemy import create_engine

engine = create_engine('sqlite:///olist.db')
result = query_revenue_by_month_year(engine)
print(result.result.head())
"
```

---

## ğŸ”§ SoluciÃ³n de Problemas Comunes

### Error: "Module not found"

```bash
# Asegurar que estÃ¡s en el directorio correcto
pwd
ls src/

# Verificar PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

### Error: "Database locked"

```bash
# Cerrar todas las conexiones a SQLite
rm -f olist.db-journal
```

### Error: "API timeout"

```bash
# Verificar conexiÃ³n a internet
ping date.nager.at

# Aumentar timeout en src/extract.py
response = requests.get(url, timeout=30)
```

### Error: "Airflow DAG no aparece"

```bash
# Verificar ruta de DAGs
echo $AIRFLOW_HOME
ls $AIRFLOW_HOME/dags/

# Verificar sintaxis del DAG
python airflow_home/dags/elt_olist_dag.py
```

---

## ğŸ“ˆ MÃ©tricas de Ã‰xito y Resultados Esperados

### Datos de Entrada

- **~100,000 Ã³rdenes** (2016-2018)
- **~50,000 clientes Ãºnicos**
- **~3,000 vendedores**
- **73 categorÃ­as de productos**
- **~27 estados de Brasil**

### Resultados Esperados

- Base de datos SQLite con 9+ tablas
- 7+ consultas SQL ejecutadas exitosamente
- 5+ visualizaciones generadas
- Todos los tests pasando (100% success rate)
- Pipeline completo en <10 minutos

### KPIs del Pipeline

- **Tiempo de extracciÃ³n**: <2 minutos
- **Tiempo de carga**: <1 minuto
- **Tiempo de transformaciÃ³n**: <3 minutos
- **Cobertura de pruebas**: >90%
- **Calidad de datos**: 0 valores nulos en campos crÃ­ticos

---

## ğŸ“ Aprendizajes y Mejores PrÃ¡cticas

### TÃ©cnicas de ETL Aprendidas

1. **ExtracciÃ³n multi-fuente**: CSV + API REST
2. **TransformaciÃ³n con SQL**: Queries complejas y eficientes
3. **Carga incremental**: Estrategias de reemplazo vs append
4. **OrquestaciÃ³n**: Dependency management con Airflow
5. **Testing**: Pruebas automatizadas para pipelines de datos

### Mejores PrÃ¡cticas Implementadas

- **SeparaciÃ³n de responsabilidades**: Cada mÃ³dulo tiene una funciÃ³n especÃ­fica
- **ConfiguraciÃ³n centralizada**: Un solo lugar para todas las configuraciones
- **GestiÃ³n de errores**: Try-catch robusto en puntos crÃ­ticos
- **DocumentaciÃ³n**: Docstrings y comentarios explicativos
- **Versionado**: Control de versiones con git
- **Entornos virtuales**: Aislamiento de dependencias

### CÃ³digo Limpio

- **PEP 8 compliance**: Formateo con Black
- **Type hints**: Anotaciones de tipos para mejor legibilidad
- **Nombres descriptivos**: Variables y funciones auto-explicativas
- **Funciones pequeÃ±as**: Single responsibility principle
- **DRY principle**: No repeticiÃ³n de cÃ³digo

---

## ğŸš€ Extensiones Futuras

### Mejoras TÃ©cnicas

1. **Data Quality**: Implementar validaciones de calidad de datos
2. **Incremental Loading**: Carga incremental instead de full replace
3. **Data Lineage**: Tracking del origen y transformaciones de datos
4. **Monitoring**: Alertas y mÃ©tricas de rendimiento del pipeline
5. **Scaling**: MigraciÃ³n a herramientas como Apache Spark para big data

### Mejoras de Negocio

1. **Real-time Analytics**: Stream processing para anÃ¡lisis en tiempo real
2. **ML Integration**: Modelos de machine learning para predicciones
3. **Dashboard Interactivo**: Herramientas como Tableau o Power BI
4. **Data Catalog**: DocumentaciÃ³n automÃ¡tica de datasets
5. **Multi-tenant**: Support para mÃºltiples clientes/regiones

### Infraestructura

1. **ContainerizaciÃ³n**: Docker para deployment consistente
2. **Cloud Migration**: AWS/GCP/Azure para escalabilidad
3. **CI/CD**: Pipeline de deployment automatizado
4. **Infrastructure as Code**: Terraform para gestiÃ³n de recursos
5. **Data Lake**: Arquitectura moderna de datos para diferentes formatos

---

## ğŸ“š Recursos Adicionales

### DocumentaciÃ³n TÃ©cnica

- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [SQLAlchemy Documentation](https://docs.sqlalchemy.org/)
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Jupyter Notebook Documentation](https://jupyter-notebook.readthedocs.io/)

### Cursos y Tutoriales

- [Real Python - ETL Tutorials](https://realpython.com/search?q=etl)
- [DataCamp - Data Engineering Track](https://www.datacamp.com/tracks/data-engineer-with-python)
- [Coursera - Data Engineering Courses](https://www.coursera.org/search?query=data%20engineering)

### Herramientas Complementarias

- **DBeaver**: IDE visual para bases de datos
- **Postman**: Testing de APIs
- **Git**: Control de versiones
- **Docker**: ContainerizaciÃ³n
- **VS Code**: Editor de cÃ³digo recomendado

---

## âœ… Checklist de FinalizaciÃ³n

### Desarrollo Completado

- [ ] Todos los TODOs implementados en `src/extract.py`
- [ ] Todos los TODOs implementados en `src/load.py`
- [ ] Todas las consultas SQL completadas en `queries/`
- [ ] Notebook `ETLProject.ipynb` ejecutado sin errores
- [ ] DAG de Airflow funcionando correctamente

### Testing y Calidad

- [ ] Todas las pruebas pasando (`pytest tests/`)
- [ ] CÃ³digo formateado con Black (`black --line-length=88 .`)
- [ ] No warnings o errors en la ejecuciÃ³n
- [ ] DocumentaciÃ³n actualizada

### Entregables

- [ ] Base de datos `olist.db` generada correctamente
- [ ] Archivos de exportaciÃ³n en `exports/` creados
- [ ] Visualizaciones en `images/` generadas
- [ ] Cache de Airflow funcionando (`.airflow_cache/`)

### DocumentaciÃ³n

- [ ] README.md revisado y actualizado
- [ ] Esta guÃ­a completa creada y revisada
- [ ] Comentarios en cÃ³digo son claros y Ãºtiles
- [ ] Instrucciones de instalaciÃ³n verificadas

---

**Â¡Felicidades!** ğŸ‰ Has completado un proyecto completo de Data Engineering con un pipeline ETL robusto, pruebas automatizadas, orquestaciÃ³n con Airflow y visualizaciones de datos. Este proyecto demuestra competencias clave en el anÃ¡lisis de datos empresariales y ingenierÃ­a de datos moderna.

---

_Ãšltima actualizaciÃ³n: Septiembre 2025_
_Proyecto: ETL E-Commerce Olist - Inteligencia de Negocios_
_Universidad: [Tu Universidad]_
_Curso: Inteligencia de Negocios_
